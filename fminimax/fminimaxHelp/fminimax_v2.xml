<?xml version="1.0" encoding="UTF-8"?>

<!--
 *
 * This help file was generated from fminimax_v2.sci using help_from_sci().
 *
 -->

<refentry version="5.0-subset Scilab" xml:id="fminimax_v2" xml:lang="en"
          xmlns="http://docbook.org/ns/docbook"
          xmlns:xlink="http://www.w3.org/1999/xlink"
          xmlns:svg="http://www.w3.org/2000/svg"
          xmlns:ns3="http://www.w3.org/1999/xhtml"
          xmlns:mml="http://www.w3.org/1998/Math/MathML"
          xmlns:scilab="http://www.scilab.org"
          xmlns:db="http://docbook.org/ns/docbook">

  <refnamediv>
    <refname>fminimax_v2</refname>
    <refpurpose>Solves minimax constraint problem</refpurpose>
  </refnamediv>


<refsynopsisdiv>
   <title>Calling Sequence</title>
   <synopsis>
   x = fminimax(fun,x0)
   x = fminimax(fun,x0,A,b)
   x = fminimax(fun,x0,A,b,Aeq,beq)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub,nonlinfun)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub,nonlinfun,options)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub,nonlinfun,options,fGrad)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub,nonlinfun,options,cGrad)
   x = fminimax(fun,x0,A,b,Aeq,beq,lb,ub,nonlinfun,options,fGrad,cGrad)
   [x, fval] = fmincon(.....)
   [x, fval, maxfval]= fmincon(.....)
   [x, fval, maxfval, exitflag]= fmincon(.....)
   [x, fval, maxfval, exitflag, output]= fmincon(.....)
   [x, fval, maxfval, exitflag, output, lambda]= fmincon(.....)
   
   </synopsis>
</refsynopsisdiv>

<refsection>
   <title>Parameters</title>
   <variablelist>
   <varlistentry><term>fun:</term>
      <listitem><para> The function to be minimized. fun is a function that accepts a vector x and returns a vector F, the objective functions evaluated at x.</para></listitem></varlistentry>
   <varlistentry><term>x0:</term>
      <listitem><para> a nx1 or 1xn matrix of doubles, where n is the number of variables, the initial guess for the optimization algorithm</para></listitem></varlistentry>
   <varlistentry><term>A:</term>
      <listitem><para> a nil x n matrix of doubles, where n is the number of variables and nil is the number of linear inequalities. If A==[] and b==[], it is assumed that there is no linear inequality constraints. If (A==[] &amp; b&lt;&gt;[]), fminimax generates an error (the same happens if (A&lt;&gt;[] &amp; b==[]))</para></listitem></varlistentry>
   <varlistentry><term>b:</term>
      <listitem><para> a nil x 1 matrix of doubles, where nil is the number of linear inequalities</para></listitem></varlistentry>
   <varlistentry><term>Aeq:</term>
      <listitem><para> a nel x n matrix of doubles, where n is the number of variables and nel is the number of linear equalities. If Aeq==[] and beq==[], it is assumed that there is no linear equality constraints. If (Aeq==[] &amp; beq&lt;&gt;[]), fminimax generates an error (the same happens if (Aeq&lt;&gt;[] &amp; beq==[]))</para></listitem></varlistentry>
   <varlistentry><term>beq:</term>
      <listitem><para> a nel x 1 matrix of doubles, where nel is the number of linear equalities</para></listitem></varlistentry>
   <varlistentry><term>lb:</term>
      <listitem><para> a nx1 or 1xn matrix of doubles, where n is the number of variables. The lower bound for x. If lb==[], then the lower bound is automatically set to -inf</para></listitem></varlistentry>
   <varlistentry><term>ub:</term>
      <listitem><para> a nx1 or 1xn matrix of doubles, where n is the number of variables. The upper bound for x. If ub==[], then the upper bound is automatically set to +inf</para></listitem></varlistentry>
   <varlistentry><term>nonlinfun:</term>
      <listitem><para> function that computes the nonlinear inequality constraints c(x) &lt;= 0 and nonlinear equality constraints ceq(x) = 0.</para></listitem></varlistentry>
   <varlistentry><term>x:</term>
      <listitem><para> a nx1 matrix of doubles, the computed solution of the optimization problem</para></listitem></varlistentry>
   <varlistentry><term>fval:</term>
      <listitem><para> a vector of doubles, the value of fun at x</para></listitem></varlistentry>
   <varlistentry><term>maxfval:</term>
      <listitem><para> a 1x1 matrix of doubles, the maximum value in vector fval</para></listitem></varlistentry>
   <varlistentry><term>exitflag:</term>
      <listitem><para> a 1x1 matrix of floating point integers, the exit status</para></listitem></varlistentry>
   <varlistentry><term>output:</term>
      <listitem><para> a struct, the details of the optimization process</para></listitem></varlistentry>
   <varlistentry><term>lambda:</term>
      <listitem><para> a struct, the Lagrange multipliers at optimum</para></listitem></varlistentry>
   <varlistentry><term>options:</term>
      <listitem><para> an optional struct, as provided by optimset</para></listitem></varlistentry>
   <varlistentry><term>fGrad:</term>
      <listitem><para> n x l where l is the size of objective function fun. It is the derivative of fun.</para></listitem></varlistentry>
   <varlistentry><term>cGrad:</term>
      <listitem><para> user definition for gradient of nonlinfun. See examples for details.</para></listitem></varlistentry>
   </variablelist>
</refsection>

<refsection>
   <title>Description</title>
   <para>
fminimax minimizes the worst-case (largest) value of a set of multivariable functions, starting at an initial estimate. This is generally referred to as the minimax problem.
   </para>
   <para>
<latex>
\min_{x} \max_{i} F_{i}(x)\: \textrm{such that} \:\begin{cases}
&amp; c(x) \leq 0 \\
&amp; ceq(x) = 0 \\
&amp; A.x \leq b \\
&amp; Aeq.x = beq \\
&amp; lb \leq x \leq ub
\end{cases}
</latex>
   </para>
   <para>
Currently, fminimax calls fmincon which uses the ip-opt algorithm.
   </para>
   <para>
max-min problems can also be solved with fminimax, using the identity
   </para>
   <para>
<latex>
\max_{x} \min_{i} F_{i}(x) = -\min_{x} \max_{i} \left( -F_{i}(x) \right)
</latex>
   </para>
   <para>
We can solve problems of the form
   </para>
   <para>
<latex>
\min_{x} \max_{i} G_{i}(x)
</latex>
   </para>
   <para>
where
   </para>
   <para>
<latex>
G_{i}(x) = \begin{cases}
&amp; \left|F_{i}(x) \right| \; 1 \leq i \leq m \\
&amp; F_{i}(x) \; \; \; \: i &gt; m
\end{cases}
</latex>
   </para>
   <para>
See the demonstrations for additional examples.
   </para>
   <para>
The objective function must have header :
<programlisting>
F = fun(x)
</programlisting>
where x is a n x 1 matrix of doubles and F is a m x 1 matrix of doubles where m is the total number of objective functions inside F.
On input, the variable x contains the current point and, on output, the variable F must contain the objective function values.
   </para>
   <para>
By default, the gradient options for fminimax are turned off and and fmincon does the gradient opproximation of fun. In case the GradObj option is off and GradConstr option is on, fminimax approximates fun gradient using numderivative toolbox.
   </para>
   <para>
If we can provide exact gradients, we should do so since it improves the convergence speed of the optimization algorithm.
   </para>
   <para>
Furthermore, we must enable the "GradObj" option with the statement :
<programlisting>
minimaxOptions = list("GradObj","ON");
</programlisting>
This will let fminimax know that the exact gradient of the objective function is known, so that it can change the calling sequence to the objective function.
   </para>
   <para>
The constraint function must have header :
<programlisting>
[c, ceq] = confun(x)
</programlisting>
where x is a n x 1 matrix of doubles, c is a nni x 1 matrix of doubles and ceq is a nne x 1 matrix of doubles (nni : number of nonlinear inequality constraints, nne : number of nonlinear equality constraints).
On input, the variable x contains the current point and, on output, the variable c must contain the nonlinear inequality constraints and ceq must contain the nonlinear equality constraints.
   </para>
   <para>
By default, the gradient options for fminimax are turned off and and fmincon does the gradient opproximation of confun. In case the GradObj option is on and GradCons option is off, fminimax approximates confun gradient using numderivative toolbox.
   </para>
   <para>
In order to use exact gradients, we must add an extra argument to fminimax. See examples for details.
   </para>
   <para>
Furthermore, we must enable the "GradConstr" option with the statement :
<programlisting>
minimaxOptions = optimset("GradCon","ON");
</programlisting>
   </para>
   <para>
The exitflag variable allows to know the status of the optimization.
<itemizedlist>
<listitem>exitflag=0 : Number of iterations exceeded minimaxOptions.MaxIter or number of function evaluations exceeded minimaxOptions.FunEvals.</listitem>
<listitem>exitflag=1 : First-order optimality measure was less than minimaxOptions.TolFun, and maximum constraint violation was less than minimaxOptions.TolCon.</listitem>
<listitem>exitflag=-1 : The output function terminated the algorithm.</listitem>
<listitem>exitflag=-2 : No feasible point was found.</listitem>
<listitem>exitflag=%nan : Other type of termination.</listitem>
</itemizedlist>
TODO : 2 : Change in x was less than minimaxOptions.TolX and maximum constraint violation was less than minimaxOptions.TolCon.
TODO : -3 : Current point x went below minimaxOptions.ObjectiveLimit and maximum constraint violation was less than minimaxOptions.TolCon.
   </para>
   <para>
The output data structure contains detailed informations about the optimization process.
It has type "struct" and contains the following fields.
<itemizedlist>
<listitem>output.iterations: the number of iterations performed during the search</listitem>
<listitem>output.funcCount: the number of function evaluations during the search</listitem>
<listitem>output.stepsize: an empty matrix</listitem>
<listitem>output.algorithm : the string containing the name of the algorithm. In the current version, algorithm="ipopt".</listitem>
<listitem>output.firstorderopt: the max-norm of the first-order KKT conditions.</listitem>
<listitem>output.constrviolation: the max-norm of the constraint violation.</listitem>
<listitem>output.cgiterations: the number of preconditionned conjugate gradient steps. In the current version, cgiterations=0.</listitem>
<listitem>output.message: a string containing a message describing the status of the optimization.</listitem>
</itemizedlist>
   </para>
   <para>
The lambda data structure contains the Lagrange multipliers at the end of optimization.
It has type "struct" and contains the following fields.
<itemizedlist>
<listitem>lambda.lower: the Lagrange multipliers for the lower bound constraints. In the current version, an empty matrix.</listitem>
<listitem>lambda.upper: the Lagrange multipliers for the upper bound constraints. In the current version, an empty matrix.</listitem>
<listitem>lambda.eqlin: the Lagrange multipliers for the linear equality constraints.</listitem>
<listitem>lambda.eqnonlin: the Lagrange multipliers for the nonlinear equality constraints.</listitem>
<listitem>lambda.ineqlin: the Lagrange multipliers for the linear inequality constraints.</listitem>
<listitem>lambda.ineqnonlin: the Lagrange multipliers for the nonlinear inequality constraints.</listitem>
</itemizedlist>
   </para>
   <para>
TODO : exitflag=2 : Change in x was less than minimaxOptions.TolX and maximum constraint violation was less than minimaxOptions.TolCon.
TODO : exitflag=-3 : Current point x went below minimaxOptions.ObjectiveLimit and maximum constraint violation was less than minimaxOptions.TolCon.
TODO : test with A, b
TODO : test with Aeq, beq
TODO : test with ceq
TODO : implement Display option (dependency on fmincon)
TODO : implement FinDiffType option (dependency on fmincon)
TODO : implement MaxFunEvals option (dependency on fmincon)
TODO : implement MaxIter option (dependency on fmincon)
TODO : implement OutputFcn option (dependency on fmincon)
TODO : implement PlotFcns option (dependency on fmincon)
TODO : implement TolCon option (dependency on fmincon)
TODO : implement TolFun option (dependency on fmincon)
TODO : implement TolX option (dependency on fmincon)
TODO : test all exitflag values
   </para>
   <para>
</para>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[
// A basic case :
// we provide only the objective function and the nonlinear constraint
// function
function f = myfun(x)
f(1)= 2*x(1)^2 + x(2)^2 - 48*x(1) - 40*x(2) + 304;     //Objectives
f(2)= -x(1)^2 - 3*x(2)^2;
f(3)= x(1) + 3*x(2) -18;
f(4)= -x(1) - x(2);
f(5)= x(1) + x(2) - 8;
endfunction

// The initial guess
x0 = [0.1,0.1];
// The expected solution : only 4 digits are guaranteed
xopt = [4 4]
fopt = [0 -64 -2 -8 0]
maxfopt = 0
// Run fminimax
[x,fval,maxfval,exitflag,output,lambda] = fminimax(myfun, x0)

   ]]></programlisting>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[
// A case where we provide the gradient of the objective
// functions and the Jacobian matrix of the constraints.
// The objective function and its gradient
function f = myfun(x)
f(1)= 2*x(1)^2 + x(2)^2 - 48*x(1) - 40*x(2) + 304;
f(2)= -x(1)^2 - 3*x(2)^2;
f(3)= x(1) + 3*x(2) -18;
f(4)= -x(1) - x(2);
f(5)= x(1) + x(2) - 8;
endfunction

// Defining gradient of myfun
function G = myfungrad(x)
G = [ 4*x(1) - 48, -2*x(1), 1, -1, 1;
2*x(2) - 40, -6*x(2), 3, -1, 1; ]'
endfunction


// The nonlinear constraints and the Jacobian
// matrix of the constraints
function [c,ceq] = confungrad(x)
// Inequality constraints
c(1) = 1.5 + x(1)*x(2) - x(1) - x(2)
c(2) = -x(1)*x(2) - 10
// No nonlinear equality constraints
ceq=[]
endfunction


// Defining gradient of confungrad
function [DC,DCeq] = cgrad(x)
// DC(:,i) = gradient of the i-th constraint
// DC = [
//   Dc1/Dx1  Dc1/Dx2
//   Dc2/Dx1  Dc2/Dx2
//   ]
DC= [
x(2)-1, -x(2)
x(1)-1, -x(1)
]'
DCeq = []'
endfunction

// Test with both gradient of objective and gradient of constraints
minimaxOptions = list("GradObj","ON","GradCon","ON");
// The initial guess
x0 = [0,10];
// The expected solution : only 4 digits are guaranteed
xopt = [0.92791 7.93551]
fopt = [6.73443  -189.778  6.73443  -8.86342  0.86342]
maxfopt = 6.73443
// Run fminimax
[x,fval,maxfval,exitflag,output] = fminimax(myfun,x0,[],[],[],[],[],[], confungrad,minimaxOptions,myfungrad,cgrad)


   ]]></programlisting>
</refsection>

<refsection>
   <title>Authors</title>
   <simplelist type="vert">
   <member>Animesh Baranawal</member>
   </simplelist>
</refsection>
</refentry>
